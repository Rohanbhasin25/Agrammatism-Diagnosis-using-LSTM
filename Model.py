# -*- coding: utf-8 -*-
"""Emotion_prediction_GeeksForGeeks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14UZWx9R8WDVrY41NEIhBfJ2LGiCEQNP4
"""

# Importing the required libraries
import keras
import numpy as np
from keras.models import Sequential,Model
from keras.layers import Dense,Bidirectional
from nltk.tokenize import word_tokenize,sent_tokenize
from keras.layers import *
from sklearn.model_selection import cross_val_score 
import nltk
import pandas as pd
nltk.download('punkt')


df=pd.read_csv('/Users/rohanbhasin/Desktop/IRIS/Programs/Classification.csv',header=None)
# The isear.csv contains rows with value 'No response'
# We need to remove such rows


feel_arr=df[1]
feel_arr[0]

feel_arr=[word_tokenize(sent) for sent in feel_arr]
print(feel_arr[0])

# Defined a function padd in which each sentence length is fixed to 100.
# If length is less than 100 , then the word- '<padd>' is append
def padd(arr):
    for i in range(100-len(arr)):
        arr.append('<pad>')
    return arr[:100]
  
# call the padd function for each sentence in feel_arr
for i in range(len(feel_arr)):
  feel_arr[i]=padd(feel_arr[i])

print(feel_arr[0])

vocab_f ='/Users/rohanbhasin/Downloads/glove.6B.50d.txt'

embeddings_index = {}
with open(vocab_f,encoding='utf8') as f:
    for line in f:
        values = line.rstrip().rsplit(' ')
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

embedded_feel_arr=[] 
for each_sentence in feel_arr:
    embedded_feel_arr.append([])
    for word in each_sentence:
        if word.lower() in embeddings_index:
            embedded_feel_arr[-1].append(embeddings_index[word.lower()])
        else:
            embedded_feel_arr[-1].append([0]*50)

print(embedded_feel_arr[0][0])

embeddings_index['happy']

X=np.array(embedded_feel_arr)
print(np.shape(X))

# Perform one-hot encoding on df[0] i.e emotion
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
Y = enc.fit_transform(np.array(df[0]).reshape(-1,1)).toarray()

# Split into train and test
from keras.layers import Embedding
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Defining the BiLSTM Model
def model(X,Y,input_size1,input_size2,output_size):
  m=Sequential()
  m.add(Bidirectional(LSTM(100,input_shape=(input_size1,input_size2))))
  m.add(Dropout(0.5))
  m.add(Dense(output_size,activation='softmax'))
  m.compile('Adam','categorical_crossentropy',['accuracy'])
  m.fit(X,Y,epochs=32, batch_size=128)
  return m

# Training the model
bilstmModel=model(X_train,Y_train,100,50,7)

bilstmModel.summary()

from keras.utils.vis_utils import plot_model
plot_model(bilstmModel, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

#Testing the model
bilstmModel.evaluate(X_test,Y_test)